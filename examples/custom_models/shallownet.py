r"""
Shallow Network
=====================

This example solves a simple binary classification problem using a basic
neural network with 1 layers.

The classification problem is generated by the make_moons dataset generator
from scikit--learn.
"""

import matplotlib.pyplot as plt
import numpy as np
import torch as pt
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons
from tqdm import tqdm

from utils.plotting import plot_decision_boundary
from skwdro.torch import robustify

seed = 42
noise=0.05
train_size, test_size = 2**9, 2**6
train_size, test_size = 2**6, 2**9 # testing against the distrib scenario
batch_size = 2**4

nb_neurons = 50
epochs, adamlr = 100, 1e-2
epochs, adamlr = 5, 1e-2

# Model
device = "cuda" if pt.cuda.is_available() else "cpu"
pt.manual_seed(seed)

class ShallowReLU(nn.Module):
    def __init__(self, in_features, hidden_units, out_features):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(in_features, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, out_features),
        )

    def forward(self, x):
        return self.seq(x)

# Data
X, y = make_moons(n_samples=train_size+test_size,
                  noise=noise,
                  random_state=seed)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    train_size=train_size,
    test_size=test_size,
    random_state=42
)

full_batch_x = pt.from_numpy(X_train).to(device)
full_batch_y = pt.from_numpy(y_train).unsqueeze(-1).to(full_batch_x)
full_batch_x_test = pt.from_numpy(X_test).to(device)
full_batch_y_test = pt.from_numpy(y_test).unsqueeze(-1).to(full_batch_x_test)

dataset = DataLoader(
    TensorDataset(
        full_batch_x,
        full_batch_y
    ),
    batch_size=batch_size
)


print("nb of batches:", len(dataset), "train samples:", train_size, "test samples:", test_size, "batch size:", batch_size)

def calcloss(model, xx, yy):
    with pt.no_grad():
        logits = model(xx)
        pred = pt.round(pt.sigmoid(logits))
        loss = loss_fn(logits, yy).mean().item()

        acc = (pred == yy).float().mean().item()*100
    return loss, acc


def train_loop(model, optimizer, lossF, robust=False):
    print(f"Robust={robust} training")
    iterator = tqdm(range(epochs), position=0, desc='Epochs', leave=False)
    test_losses, train_losses = [], []
    for epoch in iterator:
        for batch_x, batch_y in tqdm(dataset, position=1, desc='Sample', leave=False):
            model.train()
            optimizer.zero_grad()

            # loss = loss_fn(model(batch_x.squeeze()), batch_y)
            if robust:
                loss = lossF(batch_x, batch_y, reset_sampler=True)
            else:
                loss = lossF(model(batch_x), batch_y)
            loss.backward()
            optimizer.step()

        with pt.no_grad():
            train_loss, train_acc = calcloss(model, full_batch_x, full_batch_y)
            test_loss, test_acc = calcloss(model, full_batch_x_test, full_batch_y_test)
            train_losses.append(train_loss)
            test_losses.append(test_loss)
            iterator.set_description(f"Accuracy(train | test) ({train_acc:.0f}% | {test_acc:.0f}%")
            iterator.set_postfix({'loss': test_loss})
    return test_losses, train_losses

loss_fn = nn.BCEWithLogitsLoss(reduction='none')
loss = nn.BCEWithLogitsLoss(reduction='mean')
#loss_fn = nn.MSELoss(reduction='none')
#loss = nn.MSELoss(reduction='mean')

# Two identical networks
# todo: make them identical
robust_net = ShallowReLU(
    in_features=2,
    hidden_units=nb_neurons,
    out_features=1
).to(full_batch_x)

net = ShallowReLU(
    in_features=2,
    hidden_units=nb_neurons,
    out_features=1
).to(full_batch_x)

with pt.no_grad():
    robust_net.seq[0].weight.data = net.seq[0].weight.data.clone()
    robust_net.seq[2].weight.data = net.seq[2].weight.data.clone()
    robust_net.seq[0].bias.data = net.seq[0].bias.data.clone()
    robust_net.seq[2].bias.data = net.seq[2].bias.data.clone()

with pt.no_grad():
    nor_loss,_  = calcloss(net, full_batch_x_test, full_batch_y_test)
    rob_loss,_  = calcloss(robust_net, full_batch_x_test, full_batch_y_test)
    print(f"Init Test Loss: normal: {nor_loss:.3f} - robust: {rob_loss:.3f}")
    assert np.isclose(abs(nor_loss-rob_loss), 0) # fail if you forgot bias, for example..

# Robust loss def
# Define a sample batch for initialization
sample_batch_x, sample_batch_y = next(iter(dataset))
robust_loss = robustify(
    loss_fn,
    robust_net,
    pt.tensor(1e-4), # rho. Increasing rho require lower LR
    sample_batch_x, sample_batch_y,
    cost_spec="t-NC-2-2", # idk
    n_samples=16 # doesn't seem to affect much
)


robust_optimizer = pt.optim.AdamW(params=robust_loss.parameters(), lr=adamlr)
optimizer = pt.optim.AdamW(params=net.parameters(), lr=adamlr)

robust_test_losses, robust_train_losses = train_loop(robust_net, robust_optimizer, robust_loss, robust=True)
test_losses, train_losses = train_loop(net, optimizer, loss, robust=False)

with pt.no_grad():
    nor_loss,_  = calcloss(net, full_batch_x_test, full_batch_y_test)
    rob_loss,_  = calcloss(robust_net, full_batch_x_test, full_batch_y_test)
    print(f"After training, Test Loss: normal: {nor_loss:.3f} - robust: {rob_loss:.3f}")


def griddiff(modela, modelb, samples=10):
    with pt.no_grad():
        t = np.linspace(-2, 2, samples)
        Xm, Ym = np.meshgrid(t, t)
        X = pt.tensor(np.vstack((Xm.flatten(), Ym.flatten()))).T
        return nn.MSELoss()(modela(X), modelb(X)).item()

def visuals(model, test_losses, train_losses, name):
    plt.figure(figsize=(12, 12))

    plt.subplot(2, 2, 1)
    plt.title(f"Train - {name}")
    plot_decision_boundary(model, full_batch_x, full_batch_y)

    plt.subplot(2, 2, 2)
    plt.title(f"Test - {name}")
    plot_decision_boundary(model, full_batch_x_test, full_batch_y_test)

    plt.subplot(2, 1, 2)
    plt.title("Losses by epochs")
    plt.plot(test_losses, label="test")
    plt.plot(train_losses, label="train")
    plt.legend()
    plt.yscale('log')

    plt.savefig(f"main_{name}.png", dpi=100)

print("Grid output difference between the two networks:", griddiff(net, robust_net, 100))

plt.figure(figsize=(10, 10))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)
plt.savefig("data.png", dpi=100)

visuals(robust_net, robust_test_losses, robust_train_losses, "robust")
visuals(net, test_losses, train_losses, "normal")
plt.show()
