r"""
Shallow Network
=====================

This example solves a simple binary classification problem using a basic
neural network with 1 layers.

The classification problem is generated by the make_moons dataset generator
from scikit--learn.
"""

import time
import matplotlib.pyplot as plt
import numpy as np
import torch as pt
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_moons
from tqdm import tqdm
from mechanic_pytorch import mechanize # lr magic (rollbacks)
from prodigyopt import Prodigy

from utils.plotting import plot_decision_boundary
from skwdro.torch import robustify

seed = 42
noise=0.05
train_size, test_size = 2**3, 2**5 # testing against the distrib scenario
batch_size = 2**3

closeness=-0.4

nb_neurons = 100
epochs, adamlr = 500000, 1e-5
epochs, adamlr = 100000, 1e-5
opt = "adamw"

rho = 1e-3
n_samples = 2**1
n_samples = 2**11

# Model
device = "cpu"
device = "cuda" if pt.cuda.is_available() else "cpu"
pt.set_num_threads(1) # 10% perf loss for wasser but only use one core.
pt.manual_seed(seed)
rng = np.random.default_rng(seed)

class ShallowReLU(nn.Module):
    def __init__(self, in_features, hidden_units, out_features):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(in_features, hidden_units),
            nn.ReLU(),
            nn.Linear(hidden_units, out_features),
        )

    def forward(self, x):
        return self.seq(x)

# Data

def crescents_dataset(m, closeness, rng):
    def rescale(u,a,b,a1,b1): return a1 + (u-a)*(b1-a1)/(b-a)
    y = np.sign(rng.uniform(-1, 1, m))
    x = np.zeros((2,m))
    r = 1 + .1*rng.random(m) # radius
    t = np.pi/2 + np.pi*rng.random( m) # angle
    x[0,:] = r * np.sin(t)
    x[1,:] = r * np.cos(t)
    I = (y.flatten()<0) 
    x[0,I] = x[0,I] + 1
    x[1,I] = -closeness-x[1,I]
    x[0,:] = rescale(x[0,:], -1,2,  .2,.8)
    x[1,:] = rescale(x[1,:], -1,.6, .2,.8) 
    return x.T, (y.flatten()+1)/2

#X, y = make_moons(n_samples=train_size+test_size, noise=noise, random_state=seed)
#print(X.shape, y.shape)
X, y = crescents_dataset(train_size+test_size, closeness=-closeness, rng=rng)
#print(X.shape, y.shape)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    train_size=train_size,
    test_size=test_size,
    random_state=42
)

full_batch_x = pt.from_numpy(X_train).to(device)
full_batch_y = pt.from_numpy(y_train).unsqueeze(-1).to(full_batch_x)
full_batch_x_test = pt.from_numpy(X_test).to(device)
full_batch_y_test = pt.from_numpy(y_test).unsqueeze(-1).to(full_batch_x_test)

dataset = DataLoader(
    TensorDataset(
        full_batch_x,
        full_batch_y
    ),
    batch_size=batch_size
)


print("nb of batches:", len(dataset), "train samples:", train_size, "test samples:", test_size, "batch size:", batch_size, "wdro samples", n_samples)

def calcloss(model, xx, yy):
    with pt.no_grad():
        logits = model(xx)
        pred = pt.round(pt.sigmoid(logits))
        loss = loss_fn(logits, yy).mean().item()

        acc = (pred == yy).float().mean().item()*100
    return loss, acc


def train_loop(model, optimizer, lossF, robust=False, epochs=100):
    print(f"Robust={robust} training")
    iterator = tqdm(range(epochs), position=0, desc='Epochs', leave=True)
    test_losses, train_losses = [], []
    lasttime = 0
    for epoch in iterator:
        for batch_x, batch_y in dataset:#tqdm(dataset, position=1, desc='Sample', leave=False):
            model.train()
            optimizer.zero_grad()

            # loss = loss_fn(model(batch_x.squeeze()), batch_y)
            if robust:
                loss = lossF(batch_x, batch_y, reset_sampler=True)
            else:
                loss = lossF(model(batch_x), batch_y)
            loss.backward()
            optimizer.step()
        if time.time()-lasttime < 1:
            continue
        lasttime = time.time()
        print(robust_loss.lam.item())
        with pt.no_grad():
            train_loss, train_acc = calcloss(model, full_batch_x, full_batch_y)
            test_loss, test_acc = calcloss(model, full_batch_x_test, full_batch_y_test)
            train_losses.append(train_loss)
            test_losses.append(test_loss)
            iterator.set_description(f"Accuracy(train | test) ({train_acc:.0f}% | {test_acc:.0f}%")
            iterator.set_postfix({'loss': test_loss, 'train':train_loss})
    return test_losses, train_losses

loss_fn = nn.BCEWithLogitsLoss(reduction='none')
loss = nn.BCEWithLogitsLoss(reduction='mean')
#loss_fn = nn.MSELoss(reduction='none')
#loss = nn.MSELoss(reduction='mean')

# Two identical networks
# todo: make them identical
robust_net = ShallowReLU(
    in_features=2,
    hidden_units=nb_neurons,
    out_features=1
).to(full_batch_x)

net = ShallowReLU(
    in_features=2,
    hidden_units=nb_neurons,
    out_features=1
).to(full_batch_x)

with pt.no_grad():
    robust_net.seq[0].weight.data = net.seq[0].weight.data.clone()
    robust_net.seq[0].bias.data = net.seq[0].bias.data.clone()
    robust_net.seq[2].weight.data = net.seq[2].weight.data.clone()
    robust_net.seq[2].bias.data = net.seq[2].bias.data.clone()

with pt.no_grad():
    nor_loss,_  = calcloss(net, full_batch_x_test, full_batch_y_test)
    rob_loss,_  = calcloss(robust_net, full_batch_x_test, full_batch_y_test)
    print(f"Init Test Loss: normal: {nor_loss:.3f} - robust: {rob_loss:.3f}")
    assert np.isclose(abs(nor_loss-rob_loss), 0) # fail if you forgot bias, for example..

# Robust loss def
# Define a sample batch for initialization
sample_batch_x, sample_batch_y = next(iter(dataset))
robust_loss = robustify(
    loss_fn,
    robust_net,
    pt.tensor(rho), # rho. Increasing rho require lower LR
    sample_batch_x, sample_batch_y,
    cost_spec="t-NC-2-2", # idk
    n_samples=n_samples # doesn't seem to affect much
)

if opt == "adamw":
    robust_optimizer = pt.optim.AdamW(params=robust_loss.parameters(), lr=adamlr)
elif opt == "prodigy":
    robust_optimizer = Prodigy(
                        robust_loss.parameters(),
                        lr=1e0,
                        weight_decay=0,
                        safeguard_warmup=True,
                        use_bias_correction=True
                    )
elif opt == "mechanize":
    robust_optimizer = mechanize(pt.optim.AdamW)(params=robust_loss.parameters(), lr=1.)
elif opt == "self":
    robust_optimizer = robust_loss.optimizer
else:
    assert False


optimizer = pt.optim.AdamW(params=net.parameters(), lr=adamlr)
#optimizer = mechanize(pt.optim.SGD)(params=net.parameters(), lr=1)

robust_test_losses, robust_train_losses = train_loop(robust_net, robust_optimizer, robust_loss, robust=True, epochs=epochs)
#test_losses, train_losses = train_loop(net, optimizer, loss, robust=False, epochs=epochs)

with pt.no_grad():
    nor_loss,_  = calcloss(net, full_batch_x_test, full_batch_y_test)
    rob_loss,_  = calcloss(robust_net, full_batch_x_test, full_batch_y_test)
    print(f"After training, Test Loss: normal: {nor_loss:.3f} - robust: {rob_loss:.3f}")


def griddiff(modela, modelb, samples=10):
    with pt.no_grad():
        t = np.linspace(-2, 2, samples)
        Xm, Ym = np.meshgrid(t, t)
        X = pt.tensor(np.vstack((Xm.flatten(), Ym.flatten()))).T.to(device)
        return nn.MSELoss()(modela(X), modelb(X)).item()

def visuals(model, test_losses, train_losses, name):
    plt.figure(figsize=(12, 12))

    plt.subplot(2, 2, 1)
    plt.title(f"Train - {name}")
    plot_decision_boundary(model, full_batch_x, full_batch_y)

    plt.subplot(2, 2, 2)
    plt.title(f"Test - {name}")
    plot_decision_boundary(model, full_batch_x_test, full_batch_y_test)

    plt.subplot(2, 1, 2)
    plt.title("Losses by epochs")
    plt.plot(test_losses, label="test")
    plt.plot(train_losses, label="train")
    plt.legend()
    plt.yscale('log')

    plt.savefig(f"main_{name}.png", dpi=100)

#print("Grid output difference between the two networks:", griddiff(net, robust_net, 100))

plt.figure(figsize=(10, 10))
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)
plt.savefig("data.png", dpi=100)

visuals(robust_net, robust_test_losses, robust_train_losses, "robust")
#visuals(net, test_losses, train_losses, "normal")
plt.show()
