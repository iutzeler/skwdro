
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "Custom/shallow_network.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_Custom_shallow_network.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_Custom_shallow_network.py:


Shallow Neural Network
======================

We illustrate how to use a simple shallow neural network.

.. GENERATED FROM PYTHON SOURCE LINES 8-106



.. image-sg:: /Custom/images/sphx_glr_shallow_network_001.png
   :alt: shallow network
   :srcset: /Custom/images/sphx_glr_shallow_network_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/10 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s]/home/franck/.local/share/hatch/env/virtual/skwdro/d7U3xnIt/env-dev/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'
      torch.has_cuda,
    /home/franck/.local/share/hatch/env/virtual/skwdro/d7U3xnIt/env-dev/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'
      torch.has_cudnn,
    /home/franck/.local/share/hatch/env/virtual/skwdro/d7U3xnIt/env-dev/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'
      torch.has_mps,
    /home/franck/.local/share/hatch/env/virtual/skwdro/d7U3xnIt/env-dev/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'
      torch.has_mkldnn,

      0%|          | 0/2 [00:05<?, ?it/s, loss=0.62]
     50%|█████     | 1/2 [00:05<00:05,  5.77s/it, loss=0.62]
     50%|█████     | 1/2 [00:05<00:05,  5.77s/it, loss=0.67]
                                                                  0%|          | 0/10 [00:17<?, ?it/s, lambda=3.28]     10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.28]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.48]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.66]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.42]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.45]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.59]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.43]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.41]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.54]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.42]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.41]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.51]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.41]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.43]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.50]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.41]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.45]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.49]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.40]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.47]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.48]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.40]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.47]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.47]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.40]
      0%|          | 0/2 [00:00<?, ?it/s]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.49]
      0%|          | 0/2 [00:00<?, ?it/s, loss=0.47]
                                                         10%|█         | 1/10 [00:17<02:36, 17.34s/it, lambda=3.40]    100%|██████████| 10/10 [00:17<00:00,  1.26s/it, lambda=3.40]    100%|██████████| 10/10 [00:17<00:00,  1.74s/it, lambda=3.40]






|

.. code-block:: Python


    from typing import Iterable
    import matplotlib.pyplot as plt
    import torch as pt
    import torch.nn as nn
    from torch.utils.data import TensorDataset, DataLoader
    from tqdm import tqdm

    from skwdro.wrap_problem import dualize_primal_loss
    from skwdro.solvers.oracle_torch import DualLoss
    from skwdro.base.losses_torch.wrapper import WrappedPrimalLoss

    class MyShallowNet(nn.Module):
        def __init__(self, spec: list[int]) -> None:
            super(MyShallowNet, self).__init__()
            assert len(spec) > 1
            self.layers = pt.compile(nn.Sequential(
                        *([
                            nn.Sequential( # N layers
                                nn.Linear(fan_in, fan_out), # A linear layer from k to k+1
                                nn.BatchNorm1d(fan_out),
                                nn.LeakyReLU(), # A Rectified linear unit for activation
                            ) for fan_out, fan_in in zip(spec[1:-1], spec[:-2])
                        ] + [nn.Dropout1d(p=.01), nn.Linear(spec[-2], spec[-1])])
                    ))
        def forward(self, signal: pt.Tensor) -> pt.Tensor:
            if signal.dim() == 2:
                return self.layers(signal)
            elif signal.dim() == 3:
                n, m, d = signal.shape
                return self.layers(signal.flatten(start_dim=0, end_dim=1)).reshape(n, m, d)
            else: raise


    def train(dual_loss: DualLoss, dataset: Iterable[tuple[pt.Tensor, pt.Tensor]], epochs: int=10):
        optimizer = pt.optim.AdamW(dual_loss.parameters(), lr=1e-2)
        pbar = tqdm(range(epochs))

        for _ in pbar:
            # Every now and then, try to rectify the dual parameter (e.g. once per epoch).
            dual_loss.get_initial_guess_at_dual(*next(iter(dataset))) # *

            # Main train loop
            inpbar = tqdm(dataset, leave=False)
            for xi, xi_label in inpbar:
                optimizer.zero_grad()

                # Forward the batch
                loss = dual_loss(xi, xi_label, reset_sampler=True).mean()

                # Backward pass
                loss.backward()
                optimizer.step()

                inpbar.set_postfix({"loss": f"{loss.item():.2f}"})
            pbar.set_postfix({"lambda": f"{dual_loss.lam.item():.2f}"})
        assert isinstance(dual_loss.primal_loss, WrappedPrimalLoss)
        return dual_loss.primal_loss.transform

    def f(x): return pt.sin(2. * pt.pi * x)

    def main():
        device = "cuda" if pt.cuda.is_available() else "cpu"
        model = MyShallowNet([1, 10, 5, 1]).to(device)

        rho = pt.tensor(1e-1).to(device)

        x = pt.sort(pt.flatten(
            pt.linspace(0., 1., 10, device=device).unsqueeze(0)\
            + pt.randn(10, 10, device=device) * 1e-1
        ))[0]
        y = f(x) + pt.randn(100, device=device) * 2e-2
        dataset = DataLoader(TensorDataset(x.unsqueeze(-1), y.unsqueeze(-1)), batch_size=50, shuffle=True)

        # New line: "dualize" the loss
        dual_loss = dualize_primal_loss(
                nn.MSELoss(reduction='none'),
                model,
                rho,
                x.unsqueeze(-1),
                y.unsqueeze(-1)
            )

        model = train(dual_loss, dataset, 10) # type: ignore
        model.eval()

        fig, ax = plt.subplots()
        ax.scatter(x.cpu(), y.cpu(), c='g', label='train data')
        ax.plot(x.cpu(), f(x).cpu(), 'k', label='ground truth')
        ax.scatter(x.cpu(), model(x.unsqueeze(-1)).detach().cpu().squeeze(), marker='+', c='r', label='outputs')

        fig.legend()
        fig.savefig(("wdro_" if rho > 0. else "") + "net.png", transparent=True)
        plt.show()

    if __name__ == '__main__':
        pt.set_float32_matmul_precision('high')
        main()


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 23.048 seconds)


.. _sphx_glr_download_Custom_shallow_network.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: shallow_network.ipynb <shallow_network.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: shallow_network.py <shallow_network.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
