{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Shallow Neural Network\n\nWe illustrate how to use a simple shallow neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\nimport matplotlib.pyplot as plt\nimport torch as pt\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom tqdm import tqdm\n\nfrom skwdro.wrap_problem import dualize_primal_loss\nfrom skwdro.solvers.oracle_torch import DualLoss\nfrom skwdro.base.losses_torch.wrapper import WrappedPrimalLoss\n\nclass MyShallowNet(nn.Module):\n    def __init__(self, spec: list[int]) -> None:\n        super(MyShallowNet, self).__init__()\n        assert len(spec) > 1\n        self.layers = pt.compile(nn.Sequential(\n                    *([\n                        nn.Sequential( # N layers\n                            nn.Linear(fan_in, fan_out), # A linear layer from k to k+1\n                            nn.BatchNorm1d(fan_out),\n                            nn.LeakyReLU(), # A Rectified linear unit for activation\n                        ) for fan_out, fan_in in zip(spec[1:-1], spec[:-2])\n                    ] + [nn.Dropout1d(p=.01), nn.Linear(spec[-2], spec[-1])])\n                ))\n    def forward(self, signal: pt.Tensor) -> pt.Tensor:\n        if signal.dim() == 2:\n            return self.layers(signal)\n        elif signal.dim() == 3:\n            n, m, d = signal.shape\n            return self.layers(signal.flatten(start_dim=0, end_dim=1)).reshape(n, m, d)\n        else: raise\n\n\ndef train(dual_loss: DualLoss, dataset: Iterable[tuple[pt.Tensor, pt.Tensor]], epochs: int=10):\n    optimizer = pt.optim.AdamW(dual_loss.parameters(), lr=1e-2)\n    pbar = tqdm(range(epochs))\n\n    for _ in pbar:\n        # Every now and then, try to rectify the dual parameter (e.g. once per epoch).\n        dual_loss.get_initial_guess_at_dual(*next(iter(dataset))) # *\n\n        # Main train loop\n        inpbar = tqdm(dataset, leave=False)\n        for xi, xi_label in inpbar:\n            optimizer.zero_grad()\n\n            # Forward the batch\n            loss = dual_loss(xi, xi_label, reset_sampler=True).mean()\n\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n            inpbar.set_postfix({\"loss\": f\"{loss.item():.2f}\"})\n        pbar.set_postfix({\"lambda\": f\"{dual_loss.lam.item():.2f}\"})\n    assert isinstance(dual_loss.primal_loss, WrappedPrimalLoss)\n    return dual_loss.primal_loss.transform\n\ndef f(x): return pt.sin(2. * pt.pi * x)\n\ndef main():\n    device = \"cuda\" if pt.cuda.is_available() else \"cpu\"\n    model = MyShallowNet([1, 10, 5, 1]).to(device)\n\n    rho = pt.tensor(1e-1).to(device)\n\n    x = pt.sort(pt.flatten(\n        pt.linspace(0., 1., 10, device=device).unsqueeze(0)\\\n        + pt.randn(10, 10, device=device) * 1e-1\n    ))[0]\n    y = f(x) + pt.randn(100, device=device) * 2e-2\n    dataset = DataLoader(TensorDataset(x.unsqueeze(-1), y.unsqueeze(-1)), batch_size=50, shuffle=True)\n\n    # New line: \"dualize\" the loss\n    dual_loss = dualize_primal_loss(\n            nn.MSELoss(reduction='none'),\n            model,\n            rho,\n            x.unsqueeze(-1),\n            y.unsqueeze(-1)\n        )\n\n    model = train(dual_loss, dataset, 10) # type: ignore\n    model.eval()\n\n    fig, ax = plt.subplots()\n    ax.scatter(x.cpu(), y.cpu(), c='g', label='train data')\n    ax.plot(x.cpu(), f(x).cpu(), 'k', label='ground truth')\n    ax.scatter(x.cpu(), model(x.unsqueeze(-1)).detach().cpu().squeeze(), marker='+', c='r', label='outputs')\n\n    fig.legend()\n    fig.savefig((\"wdro_\" if rho > 0. else \"\") + \"net.png\", transparent=True)\n    plt.show()\n\nif __name__ == '__main__':\n    pt.set_float32_matmul_precision('high')\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}