context: {}

package:
  name: mechanic-pytorch
  version: 0.0.1

source:
- url: https://files.pythonhosted.org/packages/6d/c3/0b2fea755817314598e57eff009abb1a30680dd9c4636f5e13be46b7c776/mechanic_pytorch-0.0.1.tar.gz
  sha256: 10e16464c4764ce4e4ade92dd24eed985d5fc5e5cc664858b719ed9fe002a25d

build:
  script: python -m pip install .

requirements:
  host:
  - hatchling
  - python >=3.7
  - pip
  run:
  - python >=3.7
  - pytorch >=1.13.1

tests: []

about:
  summary: black box tuning of optimizers
  documentation: https://github.com/optimizedlearning/mechanic#readme
  license: MIT
  description: |
    Based on the paper: https://arxiv.org/abs/2306.00144
    Be aware that all experiments reported in the paper were run using the JAX version of mechanic, which is available in optax via optax.contrib.mechanize.
    Mechanic aims to remove the need for tuning a learning rate scalar (i.e. the maximum learning rate in a schedule). You can use it with any pytorch optimizer and schedule. Simply replace:
    ```python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    ```
    with:
    ```python
    from mechanic_pytorch import mechanize
    optimizer = mechanize(torch.optim.SGD)(model.parameters(), lr=1.0)
    ```
    You can set the lr to anything here.
    However, excessively small values may cause numerical precision issues.
    Mechanic's scale factor will be multiplied by the base optimizer's learning rate.
    That's it! The new optimizer should no longer require tuning the learning rate scale! That is, the optimizer should now be very robust to heavily mis-specified values of lr.

