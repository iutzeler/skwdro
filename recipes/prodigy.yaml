context: {}

package:
  name: prodigyopt
  version: '1.0'

source:
- url: https://files.pythonhosted.org/packages/dc/0e/a7660562219fef53c0fd2ddc0d0b904e09d4efb4399796d4dce8d1f2e3d5/prodigyopt-1.0.tar.gz
  sha256: cdbbd99e836fa6eb90afa49f5eb1a7760d634a15976e77e3e8114349abe910ac

build:
  script: python -m pip install .

requirements:
  host:
  - python >=3.6
  - pip
  run:
  - pytorch >=1.5.1
  - python >=3.6

tests: []

about:
  homepage: https://github.com/konstmish/prodigy
  summary: An Adam-like optimizer for neural networks with adaptive estimation of learning rate
  license: MIT
  description: |
    Let net be the neural network you want to train. Then, you can use the method as follows:
    ```python
    from prodigyopt import Prodigy
    # you can choose weight decay value based on your problem, 0 by default
    opt = Prodigy(net.parameters(), lr=1., weight_decay=weight_decay)
    ```
    Note that by default, Prodigy uses weight decay as in AdamW. If you want it to use standard regularization (as in Adam), use option decouple=False. We recommend using lr=1. (default) for all networks. If you want to force the method to estimate a smaller or larger learning rate, it is better to change the value of d_coef (1.0 by default). Values of d_coef above 1, such as 2 or 10, will force a larger estimate of the learning rate; set it to 0.5 or even 0.1 if you want a smaller learning rate.
    If you find our work useful, please consider citing our paper.
    ```bibtex
    @article{mishchenko2023prodigy,
        title={Prodigy: An Expeditiously Adaptive Parameter-Free Learner},
        author={Mishchenko, Konstantin and Defazio, Aaron},
        journal={arXiv preprint arXiv:2306.06101},
        year={2023},
        url={https://arxiv.org/pdf/2306.06101.pdf}
    }
    ```
